import numpy as np
import re

with open("/content/CBOW.txt",'r',encoding='utf-8') as f:
  corpus = f.read().lower()

corpus

tokens = re.findall(r'\b\w+\b',corpus)

tokens

vocab = sorted(set(tokens))
# (time : 2)
#          (form pair -> for items in vocab)
word2idx = {w: i for i, w in enumerate(vocab)}
# (2 : time)
#          (form pair -> for items in word2vec)
idx2word = {i: w  for w,i in word2idx.items()}

word2idx
idx2word

V = len(vocab)
window_size = 2

def one_hot_encoding(index,V):
  vec = np.zeros(V)
  vec[index] = 1
  return vec

X,Y = [], []
for i,word in enumerate(tokens):
  context=[]
  for j in range (i - window_size, i + window_size + 1):
    if j!=1 and j>=0 and j<len(tokens):
      context.append(tokens[j])
  if len(context) == 0:
    continue

  context_vec = np.mean([one_hot_encoding(word2idx[w],V) for w in context],axis=0)
  X.append(context_vec)
  Y.append(one_hot_encoding(word2idx[word],V))

X = np.array(X)
Y = np.array(Y)


X[43]

np.random.seed(42)
embed_dim = 50
learning_rate = 0.05
epochs = 200

W1 = np.random.randn(V,embed_dim)
W2 = np.random.randn(embed_dim,V)

W1



W2

def softmax(z):
    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))
    return e_z / np.sum(e_z, axis=1, keepdims=True)

for epoch in range(1, epochs+1):

  #Forward pass
  H = np.dot(X,W1)
  Z = np.dot(H, W2)
  Y_hat = softmax(Z)

  # Cross entropy loss
  loss = -np.sum(Y * np.log(Y_hat + 1e-9)) / X.shape[0]
  print(loss)

  # Backpropagation

  dZ = (Y_hat - Y) / X.shape[0]
  dW2 = np.dot(H.T, dZ)
  dH = np.dot(dZ, W2.T)
  dW1 = np.dot(X.T, dH)

  # Graident Descent Update
  W1 -= learning_rate * dW1
  W2 -= learning_rate * dW2

  if epoch % 50 == 0:
    print(f"Epoch {epoch}, Loss {loss:.4f}")
  



embeddings = W1 / (np.linalg.norm(W1, axis=1, keepdims= True) + 1e-9)

def nearest(word, k = 5):
  if word not in word2idx:
    return []
  i = word2idx[word]
  vec = embeddings[i]
  sims = np.dot(embeddings, vec)
  sims = sims / (np.linalg.norm(embeddings, axis=1) + 1e-9)
  top_k = np.argsort(-sims)[1:k+1]
  return [(idx2word[j],float(sims[j])) for j in top_k]

print(nearest("infections",5))s